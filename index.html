<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Junkai Fan</title>

  <meta name="author" content="Junkai Fan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cmu-seal-r.png">

<style>
.justify-text {
  text-align: justify;
}
</style>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Junkai Fan</name>
                  </p>
                  <!-- <p>I am a CS undergraudate at <a href="http://english.pku.edu.cn/">Peking University</a> starting from Sep. 2016. For now I'm applying for graduate school in Computer Science.
              </p> -->
                  <p>I am a Phd student at the <a href="http://www.patternrecognition.asia/">PCALab</a> of <a
                      href="https://www.njust.edu.cn/">Nanjing University of Science and Technology</a>, where I'm fortunate to be advised by Prof. 
                    <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang</a> and co-advised by Prof. <a
                      href="https://sites.google.com/view/junlineu/">Jun Li. </a>
                  </p>
                  <!-- <p>
                I am a research intern at IV-OCR Group, <a href="https://www.sensetime.com/en/">Sensetime AI</a>, starting from Oct. 2019. At Sensetime, I've worked on Form Structuralization and Image Steganography.
              </p> -->
                  <!-- <p>
                I spent last summer as an intern at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> of <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, where I'm fortunate to be advised by <a href="http://www.cs.cmu.edu/~aayushb/">Aayush Bansal</a> and <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>.
              </p> -->
                  <p>Prior to joining NJUST, I obtained my M.S degree in 2019 from <a
                      href="https://www.wzu.edu.cn/">Wenzhou University</a>, advised by Prof. <a
                      href="https://inet.wzu.edu.cn/tea_detail.html?abbr=tzz&t=0">Zhengzhou Tang</a>.</p>
                  <p style="text-align:center">
                    <a href="mailto:junkai.fan@njust.edu.cn"> Email </a> &nbsp/&nbsp
                    <!--                 <a href="data/Kangle_CV.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://github.com/fanjunkai1"> GitHub </a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=KyHsf00AAAAJ&view_op=list_works&sortby=pubdate"> Google Scholar </a> &nbsp/&nbsp
					<a href="images/wechat.jpg"> WeChat </a> 
                  </p>
                </td>
                <td style="padding:4.0%;width:40%;max-width:40%">
                  <a><img style="width:200px;max-width:100%" alt="profile photo" src="images/fanjunkai.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    <!--                 I'm interested in Multimodal Perception, including Computer Vison and Audio Processing. I'm also interested in applications of Information Theory to Maching Learning.  -->

                    <!--                 So far, my research mainly involves computer-aided creation. My research proposal of <em>'Reconstructing and Synthesizing 3D-Aware Content'</em> won 2022 Microsoft Research PhD Fellowship.  -->
                    My research is focused on computer vision and image processing. Iâ€™m particularly interested in image restoration (e.g., real-world dehazing and depth estimation). Representative papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
			
			
		  </table>
				<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
			    <td style="padding:20px;width:25%;vertical-align:middle">
					<img src="images/PGPS/PGPS.png" width="300" height="170">
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="">Physics-Guided Posterior Sampling for Diffusion-Based Real-World Dehazingand Image Enhancement</a>
                  </papertitle>
                  <br>
				  <strong>Junkai Fan</strong>,
				  <a href="https://w2kun.github.io/">Kun Wang</a>, 
				  <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>, 
				  <a href="http://www.patternrecognition.asia/qian/">Jianjun Qian</a>, 
				  <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang*</a>,
				  <a href="https://sites.google.com/view/junlineu/">Jun Li*</a>
                  <br> <br>
                  <em>Under Review</em>, 2025
                  <br>
                  <p></p>
                  <p>
				  <div class="justify-text">
				  We propose a Physics-Guided Diffusion Model for dehazing, sampling RGB and depth from a pre-trained diffusion process. 
				  It uses a Scattering-Blurred Atmospheric Scattering Model to generate high-fidelity dehazed images without aligned hazy/clear pairs, 
				  a two-stage sampling with piecewise loss for better quality and stability, and post-processing to remove JPEG artifacts amplified by dehazing.
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
          </table>
			
		<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
			    <td style="padding:20px;width:25%;vertical-align:middle">
					<img src="images/MixASNet/MixASNet.png" width="300" height="170">
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="">Daytime-mixed Non-aligned Learning for Real Nighttime Image Enhancement</a>
                  </papertitle>
                  <br>
				  <a href="https://wengjiangwei.github.io/">Jiangwei Weng</a>, 
				  <strong>Junkai Fan</strong>,
				  <a href="http://www.patternrecognition.asia/qian/">Jianjun Qian</a>, 
				  <a href="https://tyshiwo.github.io/index.html">Ying Tai</a>,
				  <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang*</a>,
				  <a href="https://sites.google.com/view/junlineu/">Jun Li*</a>
                  <br> <br>
                  <em>IEEE T-ITS</em>, 2025
                  <br>
                  <p></p>
                  <p>
				  <div class="justify-text">
				  We propose a novel nighttime image enhancement framework using daytime-mixed misaligned supervision. It aims to couple information between non-aligned daytime and nighttime image pairs. 
				  Specifically, our framework consists of a simple yet effective daytime-mixed supervised learning phase and a Retinex-based reconstruction phase.
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
         </table>
		  
		<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
			    <td style="padding:20px;width:25%;vertical-align:middle">
					<img src="images/LP-Net/LP-Net.png" width="300" height="120">
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2502.07289">Learning Inverse Laplacian Pyramid for Progressive Depth Completion</a>
                  </papertitle>
                  <br>
				  <a href="https://w2kun.github.io/">Kun Wang</a>, 
				  <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>, 
				  <strong>Junkai Fan</strong>,
				  <a href="https://sites.google.com/view/junlineu/">Jun Li*</a>, 
				  <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang*</a>
                  <br> <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <p></p>
                  <p>
				  <div class="justify-text">
				  LP-Net uses a multi-scale, progressive depth completion method via Laplacian Pyramid, starting with global scene context and refining local details with selective filtering. 
				  It achieves SOTA on indoor/outdoor datasets, is computationally efficient, and leads the KITTI leaderboard among peer-reviewed methods at submission. 
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
         </table>
		  
		<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
			    <td style="padding:20px;width:25%;vertical-align:middle">
					<img src="images/DCL/demo.gif" width="300" height="170">
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2412.11395">DCL: Depth-Centric Dehazing and Depth-Estimation from Real-World Hazy Driving Video</a>
                  </papertitle>
                  <br>
                  <strong>Junkai Fan</strong>, 
				  <a href="https://w2kun.github.io/">Kun Wang</a>, 
				  <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>, 
				  <a href="https://cschenxiang.github.io/">Xiang Chen</a>, 
				  <a href="">Shangbin Gao</a>, 
				  <a href="https://sites.google.com/view/junlineu/">Jun Li*</a>, 
				  <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang*</a>
                  <br> <br>
                  <em>AAAI</em>, 2025
                  <br>
				  <a href="projectpage/DCL/index.html">project page</a>  / 
				  <a href="https://github.com/fanjunkai1/DCL">github</a> /
				  <a href="https://www.youtube.com/watch?v=8FYw-MHksq4">video</a> /
				  <a href="images/DCL/DCL-Poster.pdf">poster</a> /
				  <a href="images/DCL/DCL-Slides.pdf">slides</a> 
                  <p></p>
                  <p>
				  <div class="justify-text">
				  We propose a novel depth-centric learning framework that combines the atmospheric scattering model(ASM) model with the brightness consistency constraint (BCC) constraint. 
				  The core idea is to use a shared depth estimation network for both ASM and BCC. 
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
         </table>
		 
		<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
			    <td style="padding:20px;width:25%;vertical-align:middle">
					<img src="images/SGDN/SGDN.png" width="300" height="120">
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2412.17496">Guided Real Image Dehazing using YCbCr Color Space</a>
                  </papertitle>
                  <br>
                  <a href="">Wengxuan Fang</a>, 
                  <strong>Junkai Fan</strong>, 
				  <a href="">Yu Zheng</a>, 
				  <a href="https://wengjiangwei.github.io/">Jiangwei Weng</a>, 
				  <a href="https://tyshiwo.github.io/index.html">Ying Tai</a>, 
				  <a href="https://sites.google.com/view/junlineu/">Jun Li*</a>, 
                  <br> <br>
                  <em>AAAI</em>, 2025
                  <br>
				  <a href="https://fiwy0527.github.io/projectpage/SGDN/index.html">project page</a>  / 
				  <a href="https://github.com/fiwy0527/AAAI_25_SGDN">github</a>
                  <p></p>
                  <p>
				  <div class="justify-text">
				  We propose a novel Structure Guided Dehazing Network (SGDN) that utilizes the superior structural properties of YCbCr features over RGB. 
				  Additionally, we introduce the Real-World Well-Aligned Haze (RW2AH) dataset, featuring diverse scenes from various geographical and climatic conditions.
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
        </table>
		 
		<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DCDepth/DCDepth.png" width="300" height="180">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

					<papertitle><a href="https://papers.nips.cc/paper_files/paper/2024/file/76bea0a1cf7bf9b78f842009f6de15a1-Paper-Conference.pdf">DCDepth: Progressive Monocular Depth Estimation in Discrete Cosine Domain</a>
					</papertitle>
					<br>
					<a href="https://w2kun.github.io/">Kun Wang</a>,  
					<a href="https://yanzq95.github.io/">Zhiqiang Yan</a>, 
					<strong>Junkai Fan</strong>, 
					<a>Wanlu Zhu</a>, 
					<a href="http://implus.github.io/">Xiang Li</a>, 
					<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
					<a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>NeurIPS</em>, 2024 
				  <br>
				  <a href="">project page</a> /
				  <a href="https://github.com/w2kun/DCDepth">github</a> /
				  <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202024/96698.png?t=1731992895.0685937">poster</a> /
				  <a href="images/DCDepth/DCDepth-Slides.pdf">slides</a>
                  <br>
                  <p></p>
                  <p>
				  <div class="justify-text">
				  DCDepth uses discrete cosine transformation on depth patches to estimate frequency coefficients, capturing local depth correlations. 
				  It separates depth into low-frequency (global structure) and high-frequency (details) components, predicting global context first and refining details progressively.
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
        </table>
		  
		<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DVD/pipeline.png" width="300">
					<img src="images/DVD/demo.gif" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2405.09996">Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance</a>
                  </papertitle>
                  <br>
                  <strong>Junkai Fan</strong>, 
				  <a href="https://wengjiangwei.github.io/">Jiangwei Weng</a>, 
				  <a href="https://w2kun.github.io/">Kun Wang</a>, 
				  <a href="https://yijun-yang.github.io/">Yijun Yang</a>, 
				  <a href="http://www.patternrecognition.asia/qian/">Jianjun Qian</a>, 
				  <a href="https://sites.google.com/view/junlineu/">Jun Li*</a>, 
				  <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang*</a>
                  <br> <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="projectpage/DVD/index.html">project page</a>  / 
				  <a href="https://github.com/fanjunkai1/DVD">github</a>  /
				  <a href="https://youtu.be/BHFVx8yv4SY">video</a> /
				  <a href="images/DVD/DVD-Poster.pdf">poster</a> 
                  <p></p>
                  <p>
				  <div class="justify-text">
				  We present an innovative video dehazing framework for real-world driving scenarios, addressing temporal and spatial misalignment 
				  challenges with non-aligned hazy/clear video pairs and a reference frame matching module.
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
          </table>
		  
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/NSDNet/demo.gif" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2303.04940v4.pdf">Non-aligned supervision for Real Image Dehazing</a>
                  </papertitle>
                  <br>
                  <strong>Junkai Fan</strong>, 
				  <a href="https://scholar.google.com/citations?hl=zh-CN&user=0wale0IAAAAJ">Fei Guo</a>, 
				  <a href="http://www.patternrecognition.asia/qian/">Jianjun Qian</a>, 
				  <a href="http://implus.github.io/">Xiang Li</a>, 
				  <a href="https://sites.google.com/view/junlineu/">Jun Li*</a>, 
				  <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang*</a>
                  <br> <br>
                  <em>IEEE TCSVT</em>, 2025
                  <br>
                  <a href="projectpage/NSDNet/index.html">project page</a> / 
				  <a href="https://github.com/fanjunkai1/NSDNet">github</a>
                  <p></p>
                  <p>
				  <div class="justify-text">
				  Given an image or video captured in a real foggy scene, our model is capable of restoring the corresponding clear scene image or video. 
				  Moreover, training our model does not require fully aligned ground truth (GT), which helps us collect real hazy scene data.
				  </div>
				  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Honors and Awards</heading>
                  <ul>
                    <li>2018, The First Prize of Scholarship, Rank (4/38), Wenzhou University; </li>
                  </ul>
                  <ul>
                    <li>2018, "Xiaoan Wang Award" for Innovation and Entrepreneurship, Rank (2/38), Wenzhou University;</li>
                  </ul>
                  <ul>
                    <li>2017, The Graduate Scientific Research Foundation of Wenzhou University, Rank (1/12), Wenzhou University;</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
		  
		  <table 
			 style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			 <tbody>
				<tr>
				  <td style="padding:20px;width:100%;vertical-align:middle">
					<heading>Academic Service</heading>
					<ul>
					  <li>Conference reviewer: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, AAAI</li>
					</ul>
					<ul>
					  <li>Journal reviewer: TCSVT, TMM, TITS</li>
					</ul>
				  </td>
				</tr>
			  </tbody>
		  </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    This webpage is fork from <a href="https://github.com/jonbarron/jonbarron_website">Jon
                      Barron</a>. Thanks to him!
                  </p>
				  <p style="text-align:center;font-size:small;">
					<a href="https://www.easycounter.com/">
					<img src="https://www.easycounter.com/counter.php?fanjunkai"
					border="0" alt="Web Counters"></a>
					<br><a href="https://www.easycounter.com/">Web Counters</a>
				  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>



</html>